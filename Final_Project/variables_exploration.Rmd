---
title: "Ensemble learning: Bagging"
author: "Victoria Lopez"
date: "15/7/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
# https://cran.r-project.org/web/packages/modeldata/modeldata.pdf
library(tidymodels)
library(modeldata)
library(baguette)
library(ggplot2)
library(reshape2)
library(reticulate)
library(patchwork)
```

Red Wine dataset 


```{r}

data <- read.csv('~/Documents/Master_4Sem/AARC/Project/winequality-red.csv', sep=";" )
glimpse(data)

```


## Data exploration 

```{r}
correlation <- data.frame(cor(data, use = "p"))
correlation
```

```{r}

cors <- function(df) { 
   # turn all three matrices (r, n, and P into a data frame)
   M <- Hmisc::rcorr(as.matrix(df))
   # return the three data frames in a list return(Mdf)
   Mdf <- map(M, ~data.frame(.x))
  }

formatted_cors <- function(df){
 cors(df) %>%
 map(~rownames_to_column(.x, var="measure1")) %>%
 map(~pivot_longer(.x, -measure1, "measure2")) %>% 
 bind_rows(.id = "id") %>%
 pivot_wider(names_from = id, values_from = value) %>%
 mutate(sig_p = ifelse(P < .05, T, F), p_if_sig = ifelse(P <.05, P, NA), r_if_sig = ifelse(P <.05, r, NA)) 
}


formatted_cors(data) %>%
 ggplot(aes(measure1, measure2, fill=r, label=round(r_if_sig,2))) +
 geom_tile() +
 labs(x = NULL, y = NULL, fill = "Pearson's\nCorrelation", title="Correlations in Wine data", subtitle="Only significant Pearson's correlation coefficients shown") + scale_fill_gradient2(mid="#FBFEF9",low="#0C6291",high="#A63446", limits=c(-1,1)) +
 geom_text() +
 theme_classic() +
 scale_x_discrete(expand=c(0,0)) +
 scale_y_discrete(expand=c(0,0)) 

```

```{r}
ggplot(data, aes(volatile.acidity))+ 
  geom_density(aes(fill=factor(quality)), alpha=0.5) +
  scale_fill_brewer(palette = "Spectral") +
  labs(title="Density plot",
       subtitle="Volatile Acidity in Wine by Quality Category",
       caption="Source: wine-quality",
       x="Fixed Acidity",
       fill="Wine Quality")

```


```{r}

ggplot(data, aes(x=fixed.acidity, y= density, color= factor(quality))) +
  geom_point(alpha = 0.4, size=.8) +
  labs(title = 'Wine Quality') +
  facet_wrap(~quality) +
  geom_smooth(method = 'lm')

```
```{r}

p1 <- ggplot(data, aes(x=density, y= fixed.acidity)) +
  geom_point(alpha = 0.4, size=.8) +
  labs(title = 'Wine Quality') +
  geom_smooth(method = 'lm')

p2 <- ggplot(data, aes(x=pH, y= fixed.acidity)) +
  geom_point(alpha = 0.4, size=.8) +
  labs(title = 'Wine Quality') +
  geom_smooth(method = 'lm')

p3 <- ggplot(data, aes(x=citric.acid, y= fixed.acidity)) +
  geom_point(alpha = 0.4, size=.8) +
  labs(title = 'Wine Quality') +
  geom_smooth(method = 'lm')

(p1+p2)/p3

```

## Random forests
The random forest approach is a bagging method where deep trees, fitted on bootstrap samples, are combined to produce an output with lower variance.

## Example: 

```{r}
library(tidyverse)
library(tidymodels)
library (randomForest)
library(MASS)
library(moderndive)
library(patchwork)
library(modeldata)
library(baguette)
library(ROCR)
library(JOUSBoost)
library(caret)
library(tree)

```


```{r}
data <- read.csv('~/Documents/Master_4Sem/AARC/Project/winequality-red.csv', sep=";" )
data <- data[, c("fixed.acidity", "citric.acid",'density','pH')]
```


```{r}
#model_rf <- rand_forest() %>% 
#  set_engine("randomForest") %>% 
#  set_mode("regression") %>% 
#  translate()
#model_rf
```


```{r}
# ntree=25
bag.wine = randomForest(fixed.acidity~., data=data,
mtry =13, ntree =1000,importance=TRUE)
bag.wine
##*** Respuesta a la actividad
```

```{r}

tree.datos<- tree(fixed.acidity~., data=data)
png('tree.png')
plot(tree.datos)
text(tree.datos, pretty=0)
dev.off()

```


```{r}
predict(bag.wine, head(data, 5))
```
```{r}
varImpPlot(bag.wine)
```
#### Bagging (regression)
stands for bootstrap aggregating. It consists of building multiple different decisin tree models from a single training data set by repeatedly using multiple bootstraped subsets of the data and averaging the models. Here, each tree is build independently to the others. 

```{r}
#control	
#A list of options generated by control_bag().
ctrl <- control_bag(var_imp = TRUE)

# `times` is low to make the examples run faster
set.seed(7687)
# Using formulas
#  multivariate adaptive regression splines (MARS)
wine_bag <- bagger(fixed.acidity ~ ., data = data, base_model = "MARS", times = 5,
                   control = ctrl)
wine_bag
```
```{r}
predictions = predict(wine_bag, new_data = data)
predictions
```

```{r}
data$predictions = predictions$.pred
g <- ggplot(data, aes(x = fixed.acidity, y = predictions)) +
  geom_point() +  geom_line(aes(x = fixed.acidity, y = fixed.acidity))

g
```

```{r}
tree_opt <- function(data, splt, y, engine2){
  # engine 
  if (engine2 == 'classification') {
    engine <- c('C5.0','classification')
  } else {
    engine <- c('MARS', 'regression')
  }
  # formula 
  form <- reformulate(termlabels = c('.'), response = c(y))
  # training split
  train <- sample(1:nrow(data), nrow(data)*splt)
  # Bagger
  ctrl <- control_bag(var_imp = TRUE)
  bagg <- bagger(form, data = data[train,], base_model = engine[1])
  # boosted tree model 
  bt_model <-
    boost_tree(
      learn_rate = 0.3,
      trees = 200,
      tree_depth = 6,
      min_n = 1,
      sample_size = 1,
      mode = engine[2]
    ) %>% set_engine("xgboost", verbose = 2) %>%  fit(form, data = data[train,])
  # randomForest
  rf_model <-
  rand_forest(trees = 1000, mtry = 5, mode = engine[2]) %>% set_engine("randomForest",
                                                                              # importance = T to have permutation score calculated
                                                                              importance = T,
                                                                              # localImp=T for randomForestExplainer(next post)
                                                                              localImp = T,) %>% fit(form, data = data[train,])
  # predictions 
  info.bag <- predict(bagg, data[-train,])
  info.bt <- predict(bt_model, data[-train,])
  info.rf <- predict(rf_model, data[-train,])
  # Metrics
  if (engine2 == 'classification'){
    acc.bag <- confusionMatrix(info.bag$.pred_class, (data[-train,]$grade))
    acc.bt <- confusionMatrix(info.bt$.pred_class, (data[-train,]$grade))
    acc.rf <- confusionMatrix(info.rf$.pred_class, (data[-train,]$grade))
    accuracy <- c(acc.bag$overall[1], acc.bt$overall[1], acc.rf$overall[1])
  } else {
    acc.bag <- sd(info.bag$.pred - data[-train,]$fixed.acidity)
    acc.bt <- sd(info.bt$.pred - data[-train,]$fixed.acidity)
    acc.rf <- sd(info.rf$.pred - data[-train,]$fixed.acidity)
    accuracy <- c(acc.bag, acc.bt, acc.rf)
  }

  # Result
  
  model <- c('Bagger', 'Boosted', 'RandomForest')
  data.frame(model, accuracy)
}
```


```{r}
tree_opt(data, .8, 'fixed.acidity', 'regression')
```
```
